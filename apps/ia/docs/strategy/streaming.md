# Streaming

O streaming no LangGraph permite que respostas sejam transmitidas em tempo real, conforme s√£o geradas, proporcionando uma experi√™ncia mais interativa e responsiva para o usu√°rio. Isso √© especialmente importante para aplica√ß√µes que precisam de feedback imediato.

## Por que Streaming √© Importante?

- **Experi√™ncia do Usu√°rio**: Feedback imediato e interativo
- **Percep√ß√£o de Performance**: Aplica√ß√£o parece mais r√°pida
- **Engajamento**: Usu√°rios permanecem envolvidos durante o processamento
- **Efici√™ncia**: N√£o precisa esperar processamento completo para come√ßar a mostrar resultados

## Exemplo Pr√°tico: Chatbot com Streaming de Respostas

```python
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict, Annotated
import operator
import asyncio
import json

# Estado do chat com streaming
class ChatState(TypedDict):
    messages: Annotated[list, operator.add]
    user_query: str
    streaming_response: str
    is_streaming: bool

# Ferramentas para o agente
@tool
def search_knowledge_base(query: str) -> str:
    """Busca informa√ß√µes na base de conhecimento"""
    # Simula busca em base de conhecimento
    knowledge = {
        "python": "Python √© uma linguagem de programa√ß√£o vers√°til e f√°cil de aprender.",
        "langchain": "LangChain √© um framework para construir aplica√ß√µes com LLMs.",
        "streaming": "Streaming permite transmiss√£o de dados em tempo real."
    }
    
    query_lower = query.lower()
    for key, value in knowledge.items():
        if key in query_lower:
            return value
    
    return "Informa√ß√£o n√£o encontrada na base de conhecimento."

@tool
def calculate_math(expression: str) -> str:
    """Calcula express√µes matem√°ticas simples"""
    try:
        # Simula c√°lculo matem√°tico
        result = eval(expression)
        return f"Resultado: {result}"
    except:
        return "Erro no c√°lculo. Verifique a express√£o."

# Configura√ß√£o do modelo com streaming
llm = init_chat_model(
    "anthropic:claude-3-7-sonnet-latest",
    temperature=0.7
)

tools = [search_knowledge_base, calculate_math]
llm_with_tools = llm.bind_tools(tools)

def streaming_chat_node(state: ChatState):
    """N√≥ principal do chat com streaming"""
    
    # Prepara mensagens para o LLM
    messages = [
        {"role": "system", "content": "Voc√™ √© um assistente √∫til. Responda de forma clara e concisa."},
        {"role": "user", "content": state["user_query"]}
    ]
    
    # Simula streaming de resposta
    response_chunks = [
        "Entendo sua pergunta sobre ",
        state["user_query"][:20] + "... ",
        "Deixe-me buscar informa√ß√µes relevantes. ",
        "Baseado no que encontrei, ",
        "posso te ajudar com isso. ",
        "Aqui est√° uma resposta detalhada: ",
        "Espero que isso seja √∫til!"
    ]
    
    return {
        "messages": [AIMessage(content="".join(response_chunks))],
        "streaming_response": "".join(response_chunks),
        "is_streaming": False
    }

def tool_execution_node(state: ChatState):
    """Executa ferramentas quando necess√°rio"""
    
    # Simula execu√ß√£o de ferramenta
    query = state["user_query"]
    
    if any(word in query.lower() for word in ["python", "langchain", "streaming"]):
        result = search_knowledge_base(query)
    elif any(op in query for op in ["+", "-", "*", "/", "="]):
        result = calculate_math(query)
    else:
        result = "N√£o identifiquei uma ferramenta espec√≠fica para usar."
    
    return {
        "messages": [AIMessage(content=f"üîß Ferramenta executada: {result}")],
        "streaming_response": state["streaming_response"] + f"\n\nüîß {result}"
    }

def route_chat(state: ChatState):
    """Roteia entre chat normal e execu√ß√£o de ferramentas"""
    query = state["user_query"].lower()
    
    # Se cont√©m palavras-chave que requerem ferramentas
    if any(word in query for word in ["python", "langchain", "streaming", "+", "-", "*", "/"]):
        return "tool_execution"
    else:
        return "streaming_chat"

def create_streaming_chat_graph():
    """Cria o grafo de chat com streaming"""
    
    builder = StateGraph(ChatState)
    
    # Adiciona n√≥s
    builder.add_node("streaming_chat", streaming_chat_node)
    builder.add_node("tool_execution", tool_execution_node)
    
    # Adiciona arestas condicionais
    builder.add_conditional_edges(
        START,
        route_chat,
        {
            "streaming_chat": "streaming_chat",
            "tool_execution": "tool_execution"
        }
    )
    
    builder.add_edge("streaming_chat", END)
    builder.add_edge("tool_execution", END)
    
    return builder.compile()

# Implementa√ß√£o de streaming real
class StreamingChatAgent:
    """Agente de chat com streaming real"""
    
    def __init__(self):
        self.graph = create_streaming_chat_graph()
    
    async def stream_response(self, query: str):
        """Streaming de resposta em tempo real"""
        
        print(f"üë§ Usu√°rio: {query}")
        print("ü§ñ Assistente: ", end="", flush=True)
        
        # Simula streaming palavra por palavra
        response_words = [
            "Entendo", "sua", "pergunta.", "Deixe-me", "processar", "isso", "para", "voc√™.",
            "Baseado", "no", "que", "analisei,", "aqui", "est√°", "minha", "resposta:"
        ]
        
        full_response = ""
        for word in response_words:
            print(word, end=" ", flush=True)
            full_response += word + " "
            await asyncio.sleep(0.3)  # Simula delay de processamento
        
        # Adiciona resposta espec√≠fica baseada na query
        if "python" in query.lower():
            print("\nüêç Python √© uma linguagem excelente para desenvolvimento!")
        elif "streaming" in query.lower():
            print("\nüì° Streaming permite respostas em tempo real!")
        else:
            print("\n‚ú® Espero ter ajudado com sua pergunta!")
        
        return full_response.strip()

# Demonstra√ß√£o de streaming com diferentes tipos de resposta
async def demonstrate_streaming():
    """Demonstra diferentes tipos de streaming"""
    
    agent = StreamingChatAgent()
    
    print("=== Demonstra√ß√£o de Streaming ===\n")
    
    # Teste 1: Streaming simples
    print("1. Streaming de Resposta Simples:")
    await agent.stream_response("O que √© Python?")
    
    print("\n" + "="*50 + "\n")
    
    # Teste 2: Streaming com ferramentas
    print("2. Streaming com Execu√ß√£o de Ferramentas:")
    await agent.stream_response("Calcule 2 + 2")
    
    print("\n" + "="*50 + "\n")
    
    # Teste 3: Streaming de resposta longa
    print("3. Streaming de Resposta Longa:")
    await agent.stream_response("Explique streaming em detalhes")

# Implementa√ß√£o de streaming com LangChain real
def demonstrate_langchain_streaming():
    """Demonstra streaming usando LangChain real"""
    
    print("\n=== Streaming com LangChain ===\n")
    
    # Configura√ß√£o do modelo com streaming
    llm = init_chat_model(
        "anthropic:claude-3-7-sonnet-latest",
        temperature=0.7
    )
    
    # Fun√ß√£o para processar streaming
    def process_streaming_response(prompt: str):
        print(f"üë§ Pergunta: {prompt}")
        print("ü§ñ Resposta (streaming): ", end="", flush=True)
        
        # Simula streaming usando yield
        response_parts = [
            "Esta √© uma resposta",
            " que est√° sendo",
            " transmitida em",
            " tempo real",
            " usando streaming.",
            " Cada parte",
            " aparece conforme",
            " √© processada!"
        ]
        
        full_response = ""
        for part in response_parts:
            print(part, end="", flush=True)
            full_response += part
            import time
            time.sleep(0.5)  # Simula delay
        
        print("\n")
        return full_response
    
    # Testa streaming
    response = process_streaming_response("Explique o que √© streaming")
    print(f"üìù Resposta completa: {response}")

# Implementa√ß√£o de streaming com WebSocket (conceitual)
class WebSocketStreamingHandler:
    """Handler para streaming via WebSocket"""
    
    def __init__(self, websocket):
        self.websocket = websocket
    
    async def send_chunk(self, chunk: str):
        """Envia chunk via WebSocket"""
        await self.websocket.send_text(json.dumps({
            "type": "chunk",
            "content": chunk
        }))
    
    async def send_complete(self, full_response: str):
        """Envia resposta completa"""
        await self.websocket.send_text(json.dumps({
            "type": "complete",
            "content": full_response
        }))
    
    async def stream_response(self, query: str):
        """Streaming completo via WebSocket"""
        
        # Simula processamento e streaming
        response_parts = [
            "Processando sua pergunta...",
            " Analisando contexto...",
            " Gerando resposta...",
            " Aqui est√° o resultado!"
        ]
        
        full_response = ""
        for part in response_parts:
            await self.send_chunk(part)
            full_response += part
            await asyncio.sleep(0.5)
        
        await self.send_complete(full_response)
        return full_response

if __name__ == "__main__":
    print("=== Demonstra√ß√£o de Streaming no LangGraph ===\n")
    
    # Demonstra√ß√£o ass√≠ncrona
    asyncio.run(demonstrate_streaming())
    
    # Demonstra√ß√£o com LangChain
    demonstrate_langchain_streaming()
    
    print("\n=== Benef√≠cios do Streaming ===")
    print("‚úÖ Experi√™ncia do usu√°rio melhorada")
    print("‚úÖ Percep√ß√£o de performance mais r√°pida")
    print("‚úÖ Maior engajamento do usu√°rio")
    print("‚úÖ Feedback imediato durante processamento")
    print("‚úÖ Possibilidade de cancelar opera√ß√µes longas")
```

## Caracter√≠sticas do Streaming

### 1. **Transmiss√£o em Tempo Real**
- Dados enviados conforme s√£o processados
- N√£o precisa esperar conclus√£o completa

### 2. **Chunking Inteligente**
- Divis√£o em peda√ßos l√≥gicos (palavras, frases, par√°grafos)
- Preserva contexto e legibilidade

### 3. **Controle de Fluxo**
- Possibilidade de pausar/retomar streaming
- Cancelamento de opera√ß√µes longas

### 4. **M√∫ltiplos Protocolos**
- HTTP Server-Sent Events (SSE)
- WebSocket para comunica√ß√£o bidirecional
- gRPC streaming

## Casos de Uso Pr√°ticos

1. **Chatbots Interativos**: Respostas aparecem conforme s√£o geradas
2. **Assistentes de C√≥digo**: Sugest√µes em tempo real durante digita√ß√£o
3. **An√°lise de Dados**: Resultados aparecem conforme s√£o processados
4. **Tradu√ß√£o em Tempo Real**: Texto traduzido palavra por palavra
5. **Gera√ß√£o de Conte√∫do**: Artigos ou c√≥digo gerados progressivamente

## Implementa√ß√£o T√©cnica

### 1. **Server-Sent Events (SSE)**
```python
# Exemplo conceitual de SSE
async def stream_sse():
    async for chunk in llm.astream(prompt):
        yield f"data: {json.dumps({'content': chunk})}\n\n"
```

### 2. **WebSocket Streaming**
```python
# Exemplo conceitual de WebSocket
async def websocket_stream(websocket):
    async for chunk in llm.astream(prompt):
        await websocket.send_text(chunk)
```

### 3. **HTTP Streaming**
```python
# Exemplo conceitual de HTTP streaming
def stream_http():
    def generate():
        for chunk in llm.stream(prompt):
            yield chunk
    return Response(generate(), mimetype='text/plain')
```

## Benef√≠cios Operacionais

- **UX Superior**: Usu√°rios veem progresso imediato
- **Redu√ß√£o de Abandono**: Menos usu√°rios desistem durante espera
- **Feedback Imediato**: Problemas detectados rapidamente
- **Escalabilidade**: Melhor uso de recursos do servidor
- **Flexibilidade**: Controle granular sobre transmiss√£o

O streaming transforma aplica√ß√µes de IA de "caixas pretas" em sistemas transparentes e interativos, essenciais para aplica√ß√µes modernas que priorizam a experi√™ncia do usu√°rio.
