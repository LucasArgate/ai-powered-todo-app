# Human-in-the-Loop

Human-in-the-Loop (HITL) no LangGraph permite que humanos intervenham em processos automatizados para valida√ß√£o, corre√ß√£o ou tomada de decis√µes cr√≠ticas. Isso √© essencial para aplica√ß√µes que requerem supervis√£o humana ou onde a precis√£o √© cr√≠tica.

## Por que Human-in-the-Loop √© Importante?

- **Qualidade**: Humanos podem validar e corrigir resultados
- **Seguran√ßa**: Interven√ß√£o em decis√µes cr√≠ticas ou sens√≠veis
- **Aprendizado**: Feedback humano melhora o sistema ao longo do tempo
- **Conformidade**: Atende regulamenta√ß√µes que exigem supervis√£o humana
- **Confian√ßa**: Usu√°rios confiam mais em sistemas com supervis√£o humana

## Exemplo Pr√°tico: Sistema de Modera√ß√£o de Conte√∫do com HITL

```python
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from typing_extensions import TypedDict, Annotated
import operator
import json
from enum import Enum

# Estados de modera√ß√£o
class ModerationStatus(str, Enum):
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    NEEDS_REVIEW = "needs_review"

# Estado do sistema de modera√ß√£o
class ModerationState(TypedDict):
    messages: Annotated[list, operator.add]
    content: str
    content_id: str
    ai_analysis: dict
    human_feedback: dict
    status: ModerationStatus
    confidence_score: float
    requires_human_review: bool

# Ferramentas de an√°lise autom√°tica
@tool
def analyze_content_safety(content: str) -> str:
    """Analisa seguran√ßa do conte√∫do usando IA"""
    
    # Simula an√°lise de conte√∫do
    risky_keywords = ["viol√™ncia", "√≥dio", "spam", "inapropriado"]
    content_lower = content.lower()
    
    risk_score = 0
    detected_issues = []
    
    for keyword in risky_keywords:
        if keyword in content_lower:
            risk_score += 0.3
            detected_issues.append(keyword)
    
    analysis = {
        "risk_score": min(risk_score, 1.0),
        "detected_issues": detected_issues,
        "recommendation": "APPROVE" if risk_score < 0.5 else "REVIEW"
    }
    
    return json.dumps(analysis)

@tool
def check_content_policy(content: str) -> str:
    """Verifica conformidade com pol√≠ticas"""
    
    # Simula verifica√ß√£o de pol√≠ticas
    policy_violations = []
    
    if len(content) > 1000:
        policy_violations.append("Conte√∫do muito longo")
    
    if content.count("!") > 5:
        policy_violations.append("Muitos pontos de exclama√ß√£o")
    
    if "@" in content and "http" in content:
        policy_violations.append("Poss√≠vel spam")
    
    policy_check = {
        "violations": policy_violations,
        "compliant": len(policy_violations) == 0
    }
    
    return json.dumps(policy_check)

def ai_analysis_node(state: ModerationState):
    """N√≥ de an√°lise autom√°tica por IA"""
    
    print(f"ü§ñ Analisando conte√∫do {state['content_id']} com IA...")
    
    # Executa an√°lises autom√°ticas
    safety_analysis = json.loads(analyze_content_safety(state["content"]))
    policy_check = json.loads(check_content_policy(state["content"]))
    
    # Calcula score de confian√ßa
    confidence = 1.0 - safety_analysis["risk_score"]
    
    # Determina se precisa de revis√£o humana
    needs_human = (
        safety_analysis["risk_score"] > 0.3 or 
        not policy_check["compliant"] or
        confidence < 0.7
    )
    
    ai_analysis = {
        "safety": safety_analysis,
        "policy": policy_check,
        "confidence": confidence,
        "needs_human_review": needs_human
    }
    
    recommendation = "AUTOMATIC_APPROVE" if not needs_human else "HUMAN_REVIEW"
    
    return {
        "messages": [AIMessage(content=f"‚úÖ An√°lise IA conclu√≠da. Recomenda√ß√£o: {recommendation}")],
        "ai_analysis": ai_analysis,
        "confidence_score": confidence,
        "requires_human_review": needs_human,
        "status": ModerationStatus.NEEDS_REVIEW if needs_human else ModerationStatus.PENDING
    }

def human_review_node(state: ModerationState):
    """N√≥ de revis√£o humana"""
    
    print(f"üë§ Revis√£o humana necess√°ria para conte√∫do {state['content_id']}")
    print(f"üìä Score de confian√ßa da IA: {state['confidence_score']:.2f}")
    print(f"‚ö†Ô∏è Problemas detectados: {state['ai_analysis']['safety']['detected_issues']}")
    
    # Simula interface de revis√£o humana
    print("\n" + "="*50)
    print("INTERFACE DE REVIS√ÉO HUMANA")
    print("="*50)
    print(f"Conte√∫do: {state['content']}")
    print(f"An√°lise IA: {state['ai_analysis']}")
    print("="*50)
    
    # Simula decis√£o humana (em produ√ß√£o, seria uma interface real)
    human_decision = simulate_human_decision(state)
    
    return {
        "messages": [AIMessage(content=f"üë§ Revis√£o humana: {human_decision['decision']}")],
        "human_feedback": human_decision,
        "status": ModerationStatus.APPROVED if human_decision["decision"] == "APPROVE" else ModerationStatus.REJECTED
    }

def simulate_human_decision(state: ModerationState):
    """Simula decis√£o humana baseada em regras"""
    
    # Simula l√≥gica de decis√£o humana
    risk_score = state["ai_analysis"]["safety"]["risk_score"]
    violations = state["ai_analysis"]["policy"]["violations"]
    
    if risk_score > 0.7 or len(violations) > 2:
        decision = "REJECT"
        reason = "Conte√∫do muito arriscado"
    elif risk_score > 0.4 or len(violations) > 0:
        decision = "APPROVE_WITH_WARNING"
        reason = "Aprovado com ressalvas"
    else:
        decision = "APPROVE"
        reason = "Conte√∫do seguro"
    
    return {
        "decision": decision,
        "reason": reason,
        "reviewer_id": "human_moderator_001",
        "timestamp": "2024-01-01T10:00:00Z"
    }

def auto_approve_node(state: ModerationState):
    """N√≥ de aprova√ß√£o autom√°tica"""
    
    print(f"‚úÖ Aprova√ß√£o autom√°tica para conte√∫do {state['content_id']}")
    
    return {
        "messages": [AIMessage(content="‚úÖ Conte√∫do aprovado automaticamente pela IA")],
        "status": ModerationStatus.APPROVED,
        "human_feedback": {
            "decision": "AUTO_APPROVE",
            "reason": "Score de confian√ßa alto, sem problemas detectados",
            "reviewer_id": "ai_system"
        }
    }

def finalize_moderation_node(state: ModerationState):
    """N√≥ final de finaliza√ß√£o"""
    
    print(f"üèÅ Finalizando modera√ß√£o para conte√∫do {state['content_id']}")
    print(f"üìã Status final: {state['status']}")
    
    if state["status"] == ModerationStatus.APPROVED:
        print("‚úÖ Conte√∫do aprovado e publicado")
    else:
        print("‚ùå Conte√∫do rejeitado")
    
    return {
        "messages": [AIMessage(content=f"üèÅ Modera√ß√£o finalizada: {state['status']}")]
    }

def route_moderation(state: ModerationState):
    """Roteia o fluxo de modera√ß√£o"""
    
    if state["requires_human_review"]:
        return "human_review"
    else:
        return "auto_approve"

def route_after_review(state: ModerationState):
    """Roteia ap√≥s revis√£o humana"""
    return "finalize"

def create_moderation_graph():
    """Cria o grafo de modera√ß√£o com HITL"""
    
    builder = StateGraph(ModerationState)
    
    # Adiciona n√≥s
    builder.add_node("ai_analysis", ai_analysis_node)
    builder.add_node("human_review", human_review_node)
    builder.add_node("auto_approve", auto_approve_node)
    builder.add_node("finalize", finalize_moderation_node)
    
    # Adiciona arestas
    builder.add_edge(START, "ai_analysis")
    
    builder.add_conditional_edges(
        "ai_analysis",
        route_moderation,
        {
            "human_review": "human_review",
            "auto_approve": "auto_approve"
        }
    )
    
    builder.add_conditional_edges(
        "human_review",
        route_after_review,
        {
            "finalize": "finalize"
        }
    )
    
    builder.add_edge("auto_approve", "finalize")
    builder.add_edge("finalize", END)
    
    return builder.compile()

# Sistema de feedback para melhoria cont√≠nua
class FeedbackLearningSystem:
    """Sistema de aprendizado com feedback humano"""
    
    def __init__(self):
        self.feedback_history = []
        self.ai_performance = {"correct": 0, "incorrect": 0}
    
    def record_feedback(self, content_id: str, ai_decision: str, human_decision: str):
        """Registra feedback humano para melhoria"""
        
        feedback = {
            "content_id": content_id,
            "ai_decision": ai_decision,
            "human_decision": human_decision,
            "agreement": ai_decision == human_decision,
            "timestamp": "2024-01-01T10:00:00Z"
        }
        
        self.feedback_history.append(feedback)
        
        if feedback["agreement"]:
            self.ai_performance["correct"] += 1
        else:
            self.ai_performance["incorrect"] += 1
        
        print(f"üìö Feedback registrado: {'‚úÖ Concord√¢ncia' if feedback['agreement'] else '‚ùå Discord√¢ncia'}")
    
    def get_ai_accuracy(self):
        """Calcula precis√£o atual da IA"""
        total = self.ai_performance["correct"] + self.ai_performance["incorrect"]
        if total == 0:
            return 0.0
        return self.ai_performance["correct"] / total
    
    def suggest_improvements(self):
        """Sugere melhorias baseadas no feedback"""
        accuracy = self.get_ai_accuracy()
        
        if accuracy < 0.8:
            return "‚ö†Ô∏è IA precisa de ajustes. Considere retreinar o modelo."
        elif accuracy < 0.9:
            return "üìà IA est√° bem, mas pode melhorar com mais dados."
        else:
            return "üéâ IA est√° performando excelentemente!"

# Demonstra√ß√£o do sistema HITL
def demonstrate_hitl_system():
    """Demonstra o sistema Human-in-the-Loop"""
    
    print("=== Sistema de Modera√ß√£o com Human-in-the-Loop ===\n")
    
    graph = create_moderation_graph()
    feedback_system = FeedbackLearningSystem()
    
    # Casos de teste
    test_cases = [
        {
            "content_id": "CONTENT_001",
            "content": "Este √© um conte√∫do seguro e apropriado para todos os p√∫blicos."
        },
        {
            "content_id": "CONTENT_002", 
            "content": "Este conte√∫do cont√©m viol√™ncia e √≥dio que n√£o deveria ser publicado."
        },
        {
            "content_id": "CONTENT_003",
            "content": "Conte√∫do com muitos pontos de exclama√ß√£o!!!!! E links suspeitos http://spam.com"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- Teste {i}: {test_case['content_id']} ---")
        
        # Estado inicial
        initial_state = {
            "messages": [],
            "content": test_case["content"],
            "content_id": test_case["content_id"],
            "ai_analysis": {},
            "human_feedback": {},
            "status": ModerationStatus.PENDING,
            "confidence_score": 0.0,
            "requires_human_review": False
        }
        
        # Executa modera√ß√£o
        result = graph.invoke(initial_state)
        
        # Simula feedback para aprendizado
        ai_decision = "APPROVE" if result["status"] == ModerationStatus.APPROVED else "REJECT"
        human_decision = "APPROVE" if "seguro" in test_case["content"] else "REJECT"
        
        feedback_system.record_feedback(
            test_case["content_id"],
            ai_decision,
            human_decision
        )
        
        print(f"üìä Precis√£o atual da IA: {feedback_system.get_ai_accuracy():.2%}")
        print(f"üí° {feedback_system.suggest_improvements()}")

# Interface de revis√£o humana (conceitual)
class HumanReviewInterface:
    """Interface para revis√£o humana"""
    
    def __init__(self):
        self.pending_reviews = []
        self.reviewers = []
    
    def add_reviewer(self, reviewer_id: str, expertise: list):
        """Adiciona revisor humano"""
        self.reviewers.append({
            "id": reviewer_id,
            "expertise": expertise,
            "active": True
        })
    
    def assign_review(self, content_id: str, content: str, ai_analysis: dict):
        """Atribui revis√£o a um revisor"""
        review = {
            "content_id": content_id,
            "content": content,
            "ai_analysis": ai_analysis,
            "assigned_to": None,
            "status": "pending"
        }
        
        self.pending_reviews.append(review)
        return review
    
    def get_review_interface(self, review_id: str):
        """Retorna interface de revis√£o"""
        return {
            "review_id": review_id,
            "actions": ["APPROVE", "REJECT", "REQUEST_CHANGES"],
            "fields": ["decision", "reason", "confidence"]
        }

if __name__ == "__main__":
    demonstrate_hitl_system()
    
    print("\n=== Benef√≠cios do Human-in-the-Loop ===")
    print("‚úÖ Qualidade superior atrav√©s de valida√ß√£o humana")
    print("‚úÖ Seguran√ßa em decis√µes cr√≠ticas")
    print("‚úÖ Aprendizado cont√≠nuo do sistema")
    print("‚úÖ Conformidade com regulamenta√ß√µes")
    print("‚úÖ Maior confian√ßa dos usu√°rios")
    print("‚úÖ Redu√ß√£o de falsos positivos/negativos")
```

## Caracter√≠sticas do Human-in-the-Loop

### 1. **Interven√ß√£o Inteligente**
- Humanos s√£o chamados apenas quando necess√°rio
- Crit√©rios claros para quando requer revis√£o humana

### 2. **Feedback Estruturado**
- Interface padronizada para feedback humano
- Coleta sistem√°tica de dados para melhoria

### 3. **Aprendizado Cont√≠nuo**
- Sistema aprende com decis√µes humanas
- Melhora precis√£o ao longo do tempo

### 4. **Auditoria Completa**
- Rastreamento de todas as interven√ß√µes humanas
- Hist√≥rico para conformidade e debugging

## Casos de Uso Pr√°ticos

1. **Modera√ß√£o de Conte√∫do**: Valida√ß√£o de posts, coment√°rios, uploads
2. **Decis√µes Financeiras**: Aprova√ß√£o de empr√©stimos, transa√ß√µes suspeitas
3. **Diagn√≥stico M√©dico**: Valida√ß√£o de diagn√≥sticos por IA
4. **Revis√£o Legal**: An√°lise de contratos e documentos jur√≠dicos
5. **Controle de Qualidade**: Inspe√ß√£o de produtos e processos

## Implementa√ß√£o de Interface Humana

### 1. **Dashboard de Revis√£o**
```python
# Exemplo conceitual de dashboard
class ReviewDashboard:
    def show_pending_reviews(self):
        return self.pending_reviews
    
    def submit_review(self, review_id, decision, reason):
        # Processa decis√£o humana
        pass
```

### 2. **Sistema de Notifica√ß√µes**
```python
# Exemplo conceitual de notifica√ß√µes
class NotificationSystem:
    def notify_reviewer(self, reviewer_id, review_data):
        # Envia notifica√ß√£o para revisor
        pass
```

### 3. **M√©tricas de Performance**
```python
# Exemplo conceitual de m√©tricas
class PerformanceMetrics:
    def calculate_human_ai_agreement(self):
        # Calcula concord√¢ncia entre IA e humanos
        pass
```

## Benef√≠cios Operacionais

- **Qualidade Garantida**: Humanos validam resultados cr√≠ticos
- **Redu√ß√£o de Riscos**: Interven√ß√£o em decis√µes sens√≠veis
- **Melhoria Cont√≠nua**: Feedback humano melhora a IA
- **Conformidade**: Atende requisitos regulat√≥rios
- **Transpar√™ncia**: Processo audit√°vel e explic√°vel
- **Confian√ßa**: Usu√°rios confiam mais no sistema

O Human-in-the-Loop transforma sistemas de IA de "caixas pretas" em sistemas colaborativos onde humanos e m√°quinas trabalham juntos para alcan√ßar resultados superiores.
